import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import joblib
import os
import warnings
warnings.filterwarnings('ignore')

from sklearn.ensemble import GradientBoostingClassifier, VotingClassifier, RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    classification_report, roc_auc_score, confusion_matrix, roc_curve
)
from sklearn.model_selection import RandomizedSearchCV, GridSearchCV, learning_curve
from sklearn.inspection import permutation_importance

class GradientBoostingModel:
    def __init__(self, X_train, X_test, y_train, y_test, feature_names=None, save_dir='models'):
        """
        Kh·ªüi t·∫°o model Gradient Boosting
        
        Parameters:
        -----------
        X_train, X_test: D·ªØ li·ªáu ƒë·∫∑c tr∆∞ng ƒë√£ chu·∫©n h√≥a
        y_train, y_test: Nh√£n t∆∞∆°ng ·ª©ng
        feature_names: T√™n c√°c ƒë·∫∑c tr∆∞ng (n·∫øu kh√¥ng c√≥ s·∫Ω t·ª± ƒë·ªông t·∫°o)
        save_dir: Th∆∞ m·ª•c l∆∞u model v√† d·ªØ li·ªáu
        """
        self.X_train = X_train
        self.X_test = X_test
        self.y_train = y_train
        self.y_test = y_test
        self.feature_names = feature_names if feature_names is not None else [f'feature_{i}' for i in range(X_train.shape[1])]
        self.save_dir = save_dir
        
        self.best_model = None
        self.best_name = None
        self.best_metrics = None
        self.feature_importance = None
        self.all_results = {}
        self.all_models = {}
        
        # T·∫°o th∆∞ m·ª•c l∆∞u tr·ªØ n·∫øu ch∆∞a c√≥
        os.makedirs(save_dir, exist_ok=True)
        os.makedirs(os.path.join(save_dir, 'datasets'), exist_ok=True)
    
    # ==================================================
    # H√ÄM L∆ØU DATASET
    # ==================================================
    def save_datasets(self, prefix='gb'):
        """
        L∆∞u c√°c dataset ƒë√£ chia (train/test) ƒë·ªÉ s·ª≠ d·ª•ng sau n√†y
        """
        try:
            # L∆∞u d·ªØ li·ªáu train
            train_data = pd.DataFrame(self.X_train, columns=self.feature_names)
            train_data['target'] = self.y_train.values if hasattr(self.y_train, 'values') else self.y_train
            train_path = os.path.join(self.save_dir, 'datasets', f'{prefix}_train_dataset.csv')
            train_data.to_csv(train_path, index=False)
            
            # L∆∞u d·ªØ li·ªáu test
            test_data = pd.DataFrame(self.X_test, columns=self.feature_names)
            test_data['target'] = self.y_test.values if hasattr(self.y_test, 'values') else self.y_test
            test_path = os.path.join(self.save_dir, 'datasets', f'{prefix}_test_dataset.csv')
            test_data.to_csv(test_path, index=False)
            
            # L∆∞u th√¥ng tin dataset
            dataset_info = {
                'train_shape': self.X_train.shape,
                'test_shape': self.X_test.shape,
                'train_samples': len(self.X_train),
                'test_samples': len(self.X_test),
                'n_features': self.X_train.shape[1],
                'train_path': train_path,
                'test_path': test_path,
                'feature_names': self.feature_names,
                'target_distribution_train': pd.Series(self.y_train).value_counts().to_dict(),
                'target_distribution_test': pd.Series(self.y_test).value_counts().to_dict()
            }
            
            info_path = os.path.join(self.save_dir, 'datasets', f'{prefix}_dataset_info.pkl')
            joblib.dump(dataset_info, info_path)
            
            print("\n" + "="*60)
            print("ƒê√É L∆ØU DATASETS TH√ÄNH C√îNG!")
            print("="*60)
            print(f"Train dataset: {train_path}")
            print(f"Test dataset: {test_path}")
            print(f"Dataset info: {info_path}")
            print(f"\nTh√¥ng tin dataset:")
            print(f"  Train shape: {self.X_train.shape}")
            print(f"  Test shape: {self.X_test.shape}")
            print(f"  S·ªë ƒë·∫∑c tr∆∞ng: {self.X_train.shape[1]}")
            
            return train_path, test_path, info_path
            
        except Exception as e:
            print(f"L·ªói khi l∆∞u datasets: {e}")
            return None, None, None
    
    def load_datasets(self, train_path, test_path):
        """
        T·∫£i datasets ƒë√£ l∆∞u
        """
        try:
            # T·∫£i train dataset
            train_data = pd.read_csv(train_path)
            self.X_train = train_data.drop('target', axis=1).values
            self.y_train = train_data['target'].values
            self.feature_names = train_data.drop('target', axis=1).columns.tolist()
            
            # T·∫£i test dataset
            test_data = pd.read_csv(test_path)
            self.X_test = test_data.drop('target', axis=1).values
            self.y_test = test_data['target'].values
            
            print("\n" + "="*60)
            print("ƒê√É T·∫¢I DATASETS TH√ÄNH C√îNG!")
            print("="*60)
            print(f"Train dataset: {train_path}")
            print(f"Test dataset: {test_path}")
            print(f"Train shape: {self.X_train.shape}")
            print(f"Test shape: {self.X_test.shape}")
            print(f"S·ªë ƒë·∫∑c tr∆∞ng: {len(self.feature_names)}")
            
            return True
            
        except Exception as e:
            print(f"L·ªói khi t·∫£i datasets: {e}")
            return False
    
    # ==================================================
    # H√ÄM HU·∫§N LUY·ªÜN MODEL C∆† B·∫¢N
    # ==================================================
    def model_gb_basic(self):
        """
        Gradient Boosting v·ªõi tham s·ªë c∆° b·∫£n
        """
        print("\n[1/6] Training Gradient Boosting c∆° b·∫£n...")
        
        model = GradientBoostingClassifier(
            n_estimators=100,
            learning_rate=0.1,
            max_depth=3,
            min_samples_split=2,
            min_samples_leaf=1,
            subsample=0.8,
            random_state=42
        )
        
        model.fit(self.X_train, self.y_train)
        y_pred = model.predict(self.X_test)
        y_proba = model.predict_proba(self.X_test)[:, 1]
        
        metrics = self._calculate_metrics(self.y_test, y_pred, y_proba)
        
        # L∆∞u feature importance
        self.feature_importance = pd.DataFrame({
            'feature': self.feature_names,
            'importance': model.feature_importances_
        }).sort_values('importance', ascending=False)
        
        print(f"   ‚úì GB c∆° b·∫£n - Accuracy: {metrics['accuracy']:.4f}")
        return model, metrics
    
    def model_gb_advanced(self):
        """
        Gradient Boosting v·ªõi tham s·ªë n√¢ng cao
        """
        print("\n[2/6] Training Gradient Boosting n√¢ng cao...")
        
        model = GradientBoostingClassifier(
            n_estimators=200,
            learning_rate=0.05,
            max_depth=5,
            min_samples_split=10,
            min_samples_leaf=5,
            subsample=0.7,
            max_features='sqrt',
            random_state=42
        )
        
        model.fit(self.X_train, self.y_train)
        y_pred = model.predict(self.X_test)
        y_proba = model.predict_proba(self.X_test)[:, 1]
        
        metrics = self._calculate_metrics(self.y_test, y_pred, y_proba)
        print(f"   ‚úì GB n√¢ng cao - Accuracy: {metrics['accuracy']:.4f}")
        return model, metrics
    
    # ==================================================
    # H√ÄM TINH CH·ªàNH THAM S·ªê
    # ==================================================
    def finetuning_randomized(self):
        """
        Tinh ch·ªânh tham s·ªë v·ªõi RandomizedSearchCV
        """
        print("\n[3/6] Tinh ch·ªânh tham s·ªë v·ªõi RandomizedSearchCV...")
        
        param_dist = {
            'n_estimators': [50, 100, 150, 200],
            'learning_rate': [0.01, 0.05, 0.1, 0.2],
            'max_depth': [3, 4, 5, 6],
            'min_samples_split': [2, 5, 10],
            'min_samples_leaf': [1, 2, 4],
            'subsample': [0.7, 0.8, 0.9, 1.0],
            'max_features': ['sqrt', 'log2', None]
        }
        
        gb = GradientBoostingClassifier(random_state=42)
        random_search = RandomizedSearchCV(
            gb, param_dist, n_iter=20, cv=3, 
            scoring='accuracy', n_jobs=-1, random_state=42, verbose=0
        )
        
        random_search.fit(self.X_train, self.y_train)
        
        best_model = random_search.best_estimator_
        y_pred = best_model.predict(self.X_test)
        y_proba = best_model.predict_proba(self.X_test)[:, 1]
        
        metrics = self._calculate_metrics(self.y_test, y_pred, y_proba)
        metrics["best_params"] = random_search.best_params_
        
        print(f"   ‚úì Best params: {random_search.best_params_}")
        print(f"   ‚úì GB RandomizedSearch - Accuracy: {metrics['accuracy']:.4f}")
        return best_model, metrics
    
    def finetuning_grid(self):
        """
        Tinh ch·ªânh tham s·ªë v·ªõi GridSearchCV
        """
        print("\n[4/6] Tinh ch·ªânh tham s·ªë v·ªõi GridSearchCV...")
        
        param_grid = {
            'n_estimators': [100, 150, 200],
            'learning_rate': [0.05, 0.1, 0.15],
            'max_depth': [3, 4, 5],
            'min_samples_split': [2, 5],
            'min_samples_leaf': [1, 2],
            'subsample': [0.8, 0.9]
        }
        
        gb = GradientBoostingClassifier(random_state=42)
        grid_search = GridSearchCV(
            gb, param_grid, cv=3, 
            scoring='accuracy', n_jobs=-1, verbose=0
        )
        
        grid_search.fit(self.X_train, self.y_train)
        
        best_model = grid_search.best_estimator_
        y_pred = best_model.predict(self.X_test)
        y_proba = best_model.predict_proba(self.X_test)[:, 1]
        
        metrics = self._calculate_metrics(self.y_test, y_pred, y_proba)
        metrics["best_params"] = grid_search.best_params_
        
        print(f"   ‚úì Best params: {grid_search.best_params_}")
        print(f"   ‚úì GB GridSearch - Accuracy: {metrics['accuracy']:.4f}")
        return best_model, metrics
    
    # ==================================================
    # H√ÄM ENSEMBLE
    # ==================================================
    def ensemble_gb(self, n_runs=5):
        """
        Ensemble v·ªõi Gradient Boosting v√† c√°c model kh√°c
        """
        print(f"\n[5/6] Training Ensemble model v·ªõi {n_runs} runs...")
        
        best_acc = 0
        best_model = None
        best_y_pred = None
        best_y_proba = None
        
        for i in range(n_runs):
            # C√°c base models
            gb1 = GradientBoostingClassifier(
                n_estimators=100, learning_rate=0.1, max_depth=3,
                random_state=42+i
            )
            
            gb2 = GradientBoostingClassifier(
                n_estimators=150, learning_rate=0.05, max_depth=5,
                random_state=42+i*2
            )
            
            rf = RandomForestClassifier(
                n_estimators=100, max_depth=5,
                random_state=42+i*3
            )
            
            lr = LogisticRegression(
                max_iter=1000, random_state=42+i*4
            )
            
            # Ensemble v·ªõi voting
            ensemble_model = VotingClassifier(
                estimators=[
                    ('gb1', gb1),
                    ('gb2', gb2),
                    ('rf', rf),
                    ('lr', lr)
                ],
                voting='soft'
            )
            
            # Train v√† evaluate
            ensemble_model.fit(self.X_train, self.y_train)
            acc = ensemble_model.score(self.X_test, self.y_test)
            
            if acc > best_acc:
                best_acc = acc
                best_model = ensemble_model
                best_y_pred = ensemble_model.predict(self.X_test)
                best_y_proba = ensemble_model.predict_proba(self.X_test)[:, 1]
                
                print(f"   Run {i+1}: New best accuracy = {acc:.4f}")
        
        print(f"\n   ‚úì Ensemble t·ªët nh·∫•t sau {n_runs} runs c√≥ accuracy = {best_acc:.4f}")
        
        metrics = self._calculate_metrics(self.y_test, best_y_pred, best_y_proba)
        return best_model, metrics
    
    def stacking_gb(self):
        """
        Stacking v·ªõi Gradient Boosting l√†m meta-learner
        """
        print("\n[6/6] Training Stacking model...")
        
        from sklearn.ensemble import StackingClassifier
        
        # Base models
        base_models = [
            ('gb1', GradientBoostingClassifier(
                n_estimators=100, learning_rate=0.1, 
                max_depth=3, random_state=42
            )),
            ('gb2', GradientBoostingClassifier(
                n_estimators=150, learning_rate=0.05,
                max_depth=5, random_state=42
            )),
            ('rf', RandomForestClassifier(
                n_estimators=100, max_depth=5,
                random_state=42
            )),
            ('lr', LogisticRegression(
                max_iter=1000, random_state=42
            ))
        ]
        
        # Meta-learner
        meta_model = LogisticRegression(max_iter=1000, random_state=42)
        
        # Stacking classifier
        stacking_model = StackingClassifier(
            estimators=base_models,
            final_estimator=meta_model,
            cv=3,
            passthrough=False
        )
        
        stacking_model.fit(self.X_train, self.y_train)
        y_pred = stacking_model.predict(self.X_test)
        y_proba = stacking_model.predict_proba(self.X_test)[:, 1]
        
        metrics = self._calculate_metrics(self.y_test, y_pred, y_proba)
        print(f"   ‚úì Stacking - Accuracy: {metrics['accuracy']:.4f}")
        return stacking_model, metrics
    
    # ==================================================
    # H√ÄM H·ªñ TR·ª¢
    # ==================================================
    def _calculate_metrics(self, y_true, y_pred, y_proba):
        """
        T√≠nh to√°n c√°c metrics ƒë√°nh gi√°
        """
        return {
            "accuracy": accuracy_score(y_true, y_pred),
            "precision": precision_score(y_true, y_pred, zero_division=0),
            "recall": recall_score(y_true, y_pred, zero_division=0),
            "f1": f1_score(y_true, y_pred, zero_division=0),
            "roc_auc": roc_auc_score(y_true, y_proba) if len(np.unique(y_true)) > 1 else 0.5,
            "y_pred": y_pred,
            "y_proba": y_proba
        }
    
    # ==================================================
    # H√ÄM CH√çNH CH·∫†Y T·∫§T C·∫¢ MODEL
    # ==================================================
    def run_models(self, use_ensemble=True, use_stacking=True, save_datasets=True):
        """
        Ch·∫°y t·∫•t c·∫£ c√°c model v√† so s√°nh k·∫øt qu·∫£
        
        Parameters:
        -----------
        use_ensemble: C√≥ s·ª≠ d·ª•ng ensemble model kh√¥ng
        use_stacking: C√≥ s·ª≠ d·ª•ng stacking model kh√¥ng
        save_datasets: C√≥ l∆∞u datasets kh√¥ng
        """
        print("\n" + "="*80)
        print("B·∫ÆT ƒê·∫¶U HU·∫§N LUY·ªÜN C√ÅC M√î H√åNH GRADIENT BOOSTING")
        print("="*80)
        
        # L∆∞u datasets n·∫øu ƒë∆∞·ª£c y√™u c·∫ßu
        if save_datasets:
            self.save_datasets()
        
        # Ch·∫°y c√°c model
        self.all_models["GB Basic"], self.all_results["GB Basic"] = self.model_gb_basic()
        self.all_models["GB Advanced"], self.all_results["GB Advanced"] = self.model_gb_advanced()
        self.all_models["GB Randomized"], self.all_results["GB Randomized"] = self.finetuning_randomized()
        self.all_models["GB GridSearch"], self.all_results["GB GridSearch"] = self.finetuning_grid()
        
        if use_ensemble:
            self.all_models["GB Ensemble"], self.all_results["GB Ensemble"] = self.ensemble_gb()
        
        if use_stacking:
            self.all_models["GB Stacking"], self.all_results["GB Stacking"] = self.stacking_gb()

        # Hi·ªÉn th·ªã b·∫£ng so s√°nh
        self._display_comparison_table()
        
        # Ch·ªçn model t·ªët nh·∫•t
        self.best_name = max(self.all_results, key=lambda x: self.all_results[x]["accuracy"])
        self.best_model = self.all_models[self.best_name]
        self.best_metrics = self.all_results[self.best_name]
        
        # Hi·ªÉn th·ªã k·∫øt qu·∫£ t·ªët nh·∫•t
        self._display_best_model()
        
        # Visualize
        self.confusion_matrix_ROC()
        self.plot_feature_importance()
        self.plot_learning_curve()
        
        # L∆∞u k·∫øt qu·∫£ so s√°nh
        self.save_comparison_results()
        
        return self.best_model, self.best_name, self.best_metrics
    
    def _display_comparison_table(self):
        """
        Hi·ªÉn th·ªã b·∫£ng so s√°nh k·∫øt qu·∫£
        """
        print("\n" + "="*80)
        print("B·∫¢NG SO S√ÅNH K·∫æT QU·∫¢ C√ÅC M√î H√åNH")
        print("="*80)
        
        # T·∫°o DataFrame
        comparison_data = []
        
        for name, metrics in self.all_results.items():
            row = {
                'Model': name,
                'Accuracy': f"{metrics['accuracy']:.4f}",
                'Precision': f"{metrics['precision']:.4f}",
                'Recall': f"{metrics['recall']:.4f}",
                'F1-Score': f"{metrics['f1']:.4f}",
                'ROC-AUC': f"{metrics['roc_auc']:.4f}"
            }
            
            if 'best_params' in metrics:
                # R√∫t g·ªçn tham s·ªë ƒë·ªÉ hi·ªÉn th·ªã
                params_str = str(metrics['best_params'])
                if len(params_str) > 50:
                    params_str = params_str[:47] + "..."
                row['Best Params'] = params_str
            
            comparison_data.append(row)
        
        comparison_df = pd.DataFrame(comparison_data)
        print(comparison_df.to_string(index=False))
        print("="*80)
    
    def _display_best_model(self):
        """
        Hi·ªÉn th·ªã th√¥ng tin model t·ªët nh·∫•t
        """
        print("\n" + "="*80)
        print("M√î H√åNH T·ªêT NH·∫§T")
        print("="*80)
        print(f"T√™n: {self.best_name}")
        print(f"Lo·∫°i: {type(self.best_model).__name__}")
        print(f"Accuracy: {self.best_metrics['accuracy']:.4f}")
        print(f"Precision: {self.best_metrics['precision']:.4f}")
        print(f"Recall: {self.best_metrics['recall']:.4f}")
        print(f"F1-Score: {self.best_metrics['f1']:.4f}")
        print(f"ROC-AUC: {self.best_metrics['roc_auc']:.4f}")
        
        if 'best_params' in self.best_metrics:
            print(f"\nTham s·ªë t·ªët nh·∫•t:")
            for param, value in self.best_metrics['best_params'].items():
                print(f"  {param}: {value}")
    
    # ==================================================
    # H√ÄM VISUALIZATION
    # ==================================================
    def confusion_matrix_ROC(self):
        """
        V·∫Ω Confusion Matrix v√† ROC Curve cho model t·ªët nh·∫•t
        """
        if self.best_model is None:
            print("Ch∆∞a c√≥ model t·ªët nh·∫•t!")
            return
        
        y_pred = self.best_metrics["y_pred"]
        y_proba = self.best_metrics["y_proba"]
        
        # Confusion Matrix
        cm = confusion_matrix(self.y_test, y_pred)
        cm_df = pd.DataFrame(
            cm,
            index=['Th·ª±c t·∫ø: Kh√¥ng b·ªánh', 'Th·ª±c t·∫ø: C√≥ b·ªánh'],
            columns=['D·ª± ƒëo√°n: Kh√¥ng b·ªánh', 'D·ª± ƒëo√°n: C√≥ b·ªánh']
        )
        
        # ROC Curve
        fpr, tpr, _ = roc_curve(self.y_test, y_proba)
        roc_auc = self.best_metrics["roc_auc"]
        
        # V·∫Ω
        fig, axes = plt.subplots(1, 2, figsize=(14, 5))
        
        # Heatmap Confusion Matrix
        sns.heatmap(cm_df, annot=True, fmt='d', cmap='Blues', ax=axes[0])
        axes[0].set_title(f'Ma tr·∫≠n nh·∫ßm l·∫´n - {self.best_name}', fontsize=14, fontweight='bold')
        axes[0].set_xlabel('D·ª± ƒëo√°n', fontsize=12)
        axes[0].set_ylabel('Th·ª±c t·∫ø', fontsize=12)
        
        # ROC Curve
        axes[1].plot(fpr, tpr, color='darkorange', lw=2, 
                    label=f'ROC curve (AUC = {roc_auc:.3f})')
        axes[1].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
        axes[1].set_xlabel('False Positive Rate', fontsize=12)
        axes[1].set_ylabel('True Positive Rate', fontsize=12)
        axes[1].set_title(f'ROC Curve - {self.best_name}', fontsize=14, fontweight='bold')
        axes[1].legend(loc="lower right")
        axes[1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.show()
    
    def plot_feature_importance(self, top_n=10):
        """
        V·∫Ω bi·ªÉu ƒë·ªì feature importance
        """
        if self.feature_importance is None:
            print("Ch∆∞a c√≥ feature importance. Vui l√≤ng train model tr∆∞·ªõc.")
            return
        
        # L·∫•y top N features
        top_features = self.feature_importance.head(top_n)
        
        plt.figure(figsize=(12, 6))
        bars = plt.barh(range(len(top_features)), top_features['importance'])
        plt.yticks(range(len(top_features)), top_features['feature'])
        plt.xlabel('ƒê·ªô quan tr·ªçng', fontsize=12)
        plt.title(f'Top {top_n} ƒê·∫∑c tr∆∞ng Quan tr·ªçng nh·∫•t ({self.best_name})', 
                 fontsize=14, fontweight='bold')
        plt.gca().invert_yaxis()
        
        # Th√™m gi√° tr·ªã tr√™n m·ªói bar
        for i, bar in enumerate(bars):
            width = bar.get_width()
            plt.text(width + 0.001, bar.get_y() + bar.get_height()/2,
                    f'{width:.3f}', ha='left', va='center')
        
        plt.tight_layout()
        plt.show()
        
        # In b·∫£ng feature importance
        print("\n" + "="*60)
        print("TOP ƒê·∫∂C TR∆ØNG QUAN TR·ªåNG NH·∫§T")
        print("="*60)
        print(top_features.to_string(index=False))
    
    def plot_learning_curve(self, cv=5):
        """
        V·∫Ω learning curve
        """
        if self.best_model is None:
            print("Ch∆∞a c√≥ model t·ªët nh·∫•t. Vui l√≤ng train model tr∆∞·ªõc.")
            return
        
        # T√≠nh learning curve
        train_sizes, train_scores, val_scores = learning_curve(
            self.best_model, self.X_train, self.y_train,
            cv=cv, n_jobs=-1,
            train_sizes=np.linspace(0.1, 1.0, 10),
            scoring='accuracy'
        )
        
        # T√≠nh mean v√† std
        train_mean = np.mean(train_scores, axis=1)
        train_std = np.std(train_scores, axis=1)
        val_mean = np.mean(val_scores, axis=1)
        val_std = np.std(val_scores, axis=1)
        
        # V·∫Ω
        plt.figure(figsize=(10, 6))
        plt.plot(train_sizes, train_mean, 'o-', color='blue', 
                label='Training accuracy', linewidth=2)
        plt.fill_between(train_sizes, train_mean - train_std,
                        train_mean + train_std, alpha=0.1, color='blue')
        
        plt.plot(train_sizes, val_mean, 'o-', color='green',
                label='Validation accuracy', linewidth=2)
        plt.fill_between(train_sizes, val_mean - val_std,
                        val_mean + val_std, alpha=0.1, color='green')
        
        plt.xlabel('S·ªë l∆∞·ª£ng m·∫´u hu·∫•n luy·ªán', fontsize=12)
        plt.ylabel('Accuracy', fontsize=12)
        plt.title(f'Learning Curve - {self.best_name}', 
                 fontsize=14, fontweight='bold')
        plt.legend(loc='best')
        plt.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.show()
    
    # ==================================================
    # H√ÄM L∆ØU K·∫æT QU·∫¢
    # ==================================================
    def save_comparison_results(self):
        """
        L∆∞u k·∫øt qu·∫£ so s√°nh c√°c model
        """
        results_path = os.path.join(self.save_dir, 'model_comparison_results.csv')
        
        # T·∫°o DataFrame t·ª´ all_results
        results_data = []
        for name, metrics in self.all_results.items():
            row = {'Model': name}
            for key, value in metrics.items():
                if key not in ['y_pred', 'y_proba']:
                    if key == 'best_params':
                        row[key] = str(value)
                    else:
                        row[key] = value
            results_data.append(row)
        
        results_df = pd.DataFrame(results_data)
        results_df.to_csv(results_path, index=False)
        
        print(f"\nƒê√£ l∆∞u k·∫øt qu·∫£ so s√°nh t·∫°i: {results_path}")
        return results_path
    
    # ==================================================
    # H√ÄM L∆ØU V√Ä T·∫¢I MODEL
    # ==================================================
    def save_model(self, model_name='best_GradientBoosting.pkl'):
        """
        L∆∞u m√¥ h√¨nh t·ªët nh·∫•t v√† c√°c th√¥ng tin li√™n quan
        """
        if self.best_model is None:
            print("Kh√¥ng c√≥ m√¥ h√¨nh ƒë·ªÉ l∆∞u!")
            return None
        
        try:
            # L∆∞u model
            model_path = os.path.join(self.save_dir, model_name)
            joblib.dump(self.best_model, model_path)
            
            # L∆∞u metrics
            metrics_path = os.path.join(self.save_dir, model_name.replace(".pkl", "_metrics.pkl"))
            joblib.dump(self.best_metrics, metrics_path)
            
            # L∆∞u feature importance
            if self.feature_importance is not None:
                feature_path = os.path.join(self.save_dir, model_name.replace(".pkl", "_features.csv"))
                self.feature_importance.to_csv(feature_path, index=False)
            
            # L∆∞u to√†n b·ªô k·∫øt qu·∫£
            all_results_path = os.path.join(self.save_dir, model_name.replace(".pkl", "_all_results.pkl"))
            joblib.dump({
                'all_results': self.all_results,
                'all_models_names': list(self.all_models.keys()),
                'best_model_name': self.best_name,
                'feature_names': self.feature_names,
                'dataset_info': {
                    'train_shape': self.X_train.shape,
                    'test_shape': self.X_test.shape,
                    'train_samples': len(self.y_train),
                    'test_samples': len(self.y_test)
                }
            }, all_results_path)
            
            print("\n" + "="*80)
            print("ƒê√É L∆ØU T·∫§T C·∫¢ TH√îNG TIN MODEL!")
            print("="*80)
            print(f" Model: {model_path}")
            print(f" Metrics: {metrics_path}")
            if self.feature_importance is not None:
                print(f"üîç Feature importance: {feature_path}")
            print(f" All results: {all_results_path}")
            print(f" Save directory: {self.save_dir}")
            
            return model_path
            
        except Exception as e:
            print(f"L·ªói khi l∆∞u model: {e}")
            return None
    
    def load_model(self, model_path):
        """
        T·∫£i m√¥ h√¨nh ƒë√£ l∆∞u
        """
        try:
            if os.path.exists(model_path):
                self.best_model = joblib.load(model_path)
                
                # T·∫£i metrics n·∫øu c√≥
                metrics_path = model_path.replace(".pkl", "_metrics.pkl")
                if os.path.exists(metrics_path):
                    self.best_metrics = joblib.load(metrics_path)
                
                # T·∫£i all results n·∫øu c√≥
                all_results_path = model_path.replace(".pkl", "_all_results.pkl")
                if os.path.exists(all_results_path):
                    all_data = joblib.load(all_results_path)
                    self.all_results = all_data.get('all_results', {})
                    self.best_name = all_data.get('best_model_name', 'Unknown')
                    self.feature_names = all_data.get('feature_names', [])
                
                print(f"\n ƒê√£ t·∫£i m√¥ h√¨nh t·ª´: {model_path}")
                print(f"   T√™n model: {self.best_name}")
                print(f"   Lo·∫°i model: {type(self.best_model).__name__}")
                
                return self.best_model
            else:
                print(f" Kh√¥ng t√¨m th·∫•y file: {model_path}")
                return None
                
        except Exception as e:
            print(f" L·ªói khi t·∫£i model: {e}")
            return None
    
    # ==================================================
    # H√ÄM D·ª∞ ƒêO√ÅN
    # ==================================================
    def predict_new(self, X_new, return_proba=True, threshold=0.5):
        """
        D·ª± ƒëo√°n tr√™n d·ªØ li·ªáu m·ªõi
        
        Parameters:
        -----------
        X_new: D·ªØ li·ªáu m·ªõi (ƒë√£ chu·∫©n h√≥a)
        return_proba: C√≥ tr·∫£ v·ªÅ x√°c su·∫•t kh√¥ng
        threshold: Ng∆∞·ª°ng ph√¢n lo·∫°i
        """
        if self.best_model is None:
            print("Vui l√≤ng train ho·∫∑c load model tr∆∞·ªõc!")
            return None
        
        try:
            # ƒê·∫£m b·∫£o X_new l√† numpy array
            if isinstance(X_new, pd.DataFrame):
                X_new = X_new.values
            
            # D·ª± ƒëo√°n
            if return_proba:
                probabilities = self.best_model.predict_proba(X_new)
                predictions = (probabilities[:, 1] >= threshold).astype(int)
                
                # T·∫°o k·∫øt qu·∫£
                results = pd.DataFrame({
                    'prediction': predictions,
                    'probability_class_0': probabilities[:, 0],
                    'probability_class_1': probabilities[:, 1],
                    'prediction_label': np.where(predictions == 1, 'C√≥ b·ªánh', 'Kh√¥ng b·ªánh')
                })
            else:
                predictions = self.best_model.predict(X_new)
                results = pd.DataFrame({
                    'prediction': predictions,
                    'prediction_label': np.where(predictions == 1, 'C√≥ b·ªánh', 'Kh√¥ng b·ªánh')
                })
            
            print(f"\n ƒê√£ d·ª± ƒëo√°n {len(predictions)} m·∫´u")
            print(f"   S·ªë m·∫´u 'C√≥ b·ªánh': {sum(predictions == 1)}")
            print(f"   S·ªë m·∫´u 'Kh√¥ng b·ªánh': {sum(predictions == 0)}")
            
            return results
            
        except Exception as e:
            print(f" L·ªói khi d·ª± ƒëo√°n: {e}")
            return None
    
    # ==================================================
    # H√ÄM IN T√ìM T·∫ÆT
    # ==================================================
    def get_model_summary(self):
        """
        In t√≥m t·∫Øt th√¥ng tin model
        """
        if self.best_model is None:
            print(" Ch∆∞a c√≥ model t·ªët nh·∫•t.")
            return
        
        print("\n" + "="*80)
        print("T√ìM T·∫ÆT M√î H√åNH GRADIENT BOOSTING")
        print("="*80)
        print(f" T√™n model: {self.best_name}")
        print(f" Lo·∫°i model: {type(self.best_model).__name__}")
        
        # Th√¥ng tin model
        if hasattr(self.best_model, 'n_estimators'):
            print(f" S·ªë c√¢y: {self.best_model.n_estimators}")
        if hasattr(self.best_model, 'learning_rate'):
            print(f" Learning rate: {self.best_model.learning_rate}")
        if hasattr(self.best_model, 'max_depth'):
            print(f" Max depth: {self.best_model.max_depth}")
        
        print(f"\n Metrics tr√™n t·∫≠p test:")
        for key, value in self.best_metrics.items():
            if key not in ['y_pred', 'y_proba', 'best_params']:
                print(f"  {key}: {value:.4f}")
        
        print(f"\n Dataset info:")
        print(f"  Train shape: {self.X_train.shape}")
        print(f"  Test shape: {self.X_test.shape}")
        print(f"  S·ªë ƒë·∫∑c tr∆∞ng: {len(self.feature_names)}")
        print(f"  Train samples: {len(self.y_train)}")
        print(f"  Test samples: {len(self.y_test)}")